{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Vr8y4W8LS_NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder and Decoder\n",
        "\n",
        "# Перерыв до 19:40\n",
        "\n",
        "Всем привет! В этом занятии мы обсудим две главные архитектуры в текстовых данных Encoder и Decoder"
      ],
      "metadata": {
        "id": "_0bx7vN1ROBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5fGtk0rRGgp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nltk\n",
        "import torchvision\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "9m4nTcrUwOdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder. Task: Part-of-Speach"
      ],
      "metadata": {
        "id": "8_x6DVSyRXtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of dataset (https://huggingface.co/datasets/conll2003)"
      ],
      "metadata": {
        "id": "hGCgvpC4RdRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll_dataset = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "KGAxCj-hLNpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll_dataset"
      ],
      "metadata": {
        "id": "w-mnZOiTLWEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll_dataset['train']"
      ],
      "metadata": {
        "id": "4-fJsCRbLmoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get num of tags, words"
      ],
      "metadata": {
        "id": "EN9cyZPsLyxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [token for sentance in conll_dataset['train']['tokens'] for token in sentance]\n",
        "all_pos_tags = [tags for sentance_pos in conll_dataset['train']['pos_tags'] for tags in sentance_pos]"
      ],
      "metadata": {
        "id": "z00roBhmLmff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_words), len(all_pos_tags)"
      ],
      "metadata": {
        "id": "PqXEbjvBu2dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example with nltk (https://www.nltk.org/book/ch05.html)"
      ],
      "metadata": {
        "id": "zamzTAXcRbx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "gJrSQ2AFvtOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am fine...\"\n",
        "nltk.pos_tag(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "ikqbEY01Lakj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try with validation part"
      ],
      "metadata": {
        "id": "v_rrg5h5Laid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(conll_dataset['validation']['tokens'][0])"
      ],
      "metadata": {
        "id": "cnp0AzkALaf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll_dataset['validation']['pos_tags'][0]"
      ],
      "metadata": {
        "id": "RNKcjCE-xN_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create Dataset and Dataloader out of conll"
      ],
      "metadata": {
        "id": "F_TYYhuhLadM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConllDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, all_words):\n",
        "      self.dataset = dataset\n",
        "      self.tokenizer = {\n",
        "          w: idx + 16\n",
        "          for idx, w in enumerate(all_words)\n",
        "      }\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = [self.tokenizer[w] for w in self.dataset['tokens'][idx]]\n",
        "        tags = self.dataset['pos_tags'][idx]\n",
        "        return tokens, tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "H5_d_rjAOB7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ConllDataset(conll_dataset['train'][:1024], all_words) # only 128 datapoints \n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "3m74IW5yOB5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(items):\n",
        "  tokens = [torch.tensor(i[0]) for i in items]\n",
        "  tags = [torch.tensor(i[1]) for i in items]\n",
        "\n",
        "  packed_tokens = torch.nn.utils.rnn.pack_sequence(tokens, enforce_sorted=False)\n",
        "  packed_tags = torch.nn.utils.rnn.pack_sequence(tags, enforce_sorted=False)\n",
        "\n",
        "  return packed_tokens, packed_tags"
      ],
      "metadata": {
        "id": "CqtLL3TU3moB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4, \n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "qctPIx3FOB2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "packed_tokens, packed_tags = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "4KffW9-F3aHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_packed_tokens = torch.nn.utils.rnn.pad_packed_sequence(packed_tokens)"
      ],
      "metadata": {
        "id": "_zFHLs3p-uxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_pad_packed_tokens = pad_packed_tokens[0].reshape(-1, 4, 1)\n",
        "emb_pad_packed_tokens"
      ],
      "metadata": {
        "id": "tZYqvtQYAM0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.utils.rnn.pack_padded_sequence(emb_pad_packed_tokens, pad_packed_tokens[1], enforce_sorted=False)"
      ],
      "metadata": {
        "id": "ScRfNM8r_WaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try to code it on RNN"
      ],
      "metadata": {
        "id": "M6_oMngaReyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN4POS(torch.nn.Module):\n",
        "    def __init__(self, num_words, num_tags, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = torch.nn.Embedding(num_words, hidden_size)\n",
        "        self.rnn = torch.nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout)\n",
        "        self.cls_head = torch.nn.Linear(hidden_size, num_tags)\n",
        "\n",
        "    def forward(self, input_ids, lenght_inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: Torch.Tensor, shape: (seq_length, batch_size, hidden_size)\n",
        "\n",
        "        To understand inputs for this module, please check rnn_padding:\n",
        "            - https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
        "            - https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "        \"\"\"\n",
        "        embs = self.embeddings(input_ids)\n",
        "\n",
        "        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(embs, lenght_inputs, enforce_sorted=False)\n",
        "        rnn_outputs, _ = self.rnn(packed_sequences)\n",
        "        unpacked_sequences, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_outputs)\n",
        "\n",
        "        return self.cls_head(unpacked_sequences)"
      ],
      "metadata": {
        "id": "aBCR4h7iLpcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN4POS(len(all_words) + 10, len(all_pos_tags))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "-xylrxxfLpaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, batch in enumerate(train_dataloader):\n",
        "    inputs, labels = batch\n",
        "\n",
        "    inputs, inputs_lenght = torch.nn.utils.rnn.pad_packed_sequence(inputs)\n",
        "    labels, _ = torch.nn.utils.rnn.pad_packed_sequence(labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs, inputs_lenght)\n",
        "    num_classes = outputs.size(-1)\n",
        "    loss = criterion(outputs.view(-1, num_classes), labels.view(-1))\n",
        "    optimizer.step()\n",
        "    print(f\"{idx:>10}: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "qV61uYJMLpXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "Na3ndyorPA3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "id": "7yvNicb2FQ2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete everythin and run torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hAbbnL7uLIK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del optimizer\n",
        "del train_dataloader\n",
        "del train_dataset\n",
        "del batch\n",
        "del inputs\n",
        "del labels\n",
        "del loss"
      ],
      "metadata": {
        "id": "LGmvdzUMOdpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MY_hnJFKPHD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder. Text Generation"
      ],
      "metadata": {
        "id": "FHl7ZdTsRlHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look at dataset (https://huggingface.co/datasets/rotten_tomatoes)"
      ],
      "metadata": {
        "id": "qjeUOmWARn_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_dataset = datasets.load_dataset(\"rotten_tomatoes\")"
      ],
      "metadata": {
        "id": "Zg7Vhok2PZlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_dataset"
      ],
      "metadata": {
        "id": "lEmZ2BlpQVsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_dataset['train']['text'][0]"
      ],
      "metadata": {
        "id": "xDo5pfVJQ8vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [w for s in rt_dataset['train']['text'] for w in s.split(' ')]"
      ],
      "metadata": {
        "id": "1TMq9syyRlxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BOS > 1\n",
        "# EOS < 2\n",
        "# padding"
      ],
      "metadata": {
        "id": "yFppSDuJSShJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get num of words"
      ],
      "metadata": {
        "id": "56t00czlSMJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_words)"
      ],
      "metadata": {
        "id": "aSeAX0OdSMHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create torch dataset"
      ],
      "metadata": {
        "id": "laP8AGP5QVph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, all_words):\n",
        "      self.dataset = dataset\n",
        "      self.tokenizer = {\n",
        "          w: idx + 16\n",
        "          for idx, w in enumerate(all_words)\n",
        "      }\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # better convert to tensor\n",
        "        tokens = [1] + [self.tokenizer[w] for w in self.dataset['text'][idx].split(' ')] + [2]\n",
        "        return torch.tensor(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset['text'])"
      ],
      "metadata": {
        "id": "SCD816iVQVnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RTDataset(rt_dataset['train'], all_words) # only 128 datapoints "
      ],
      "metadata": {
        "id": "gcDrpPA-QUke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][:5]"
      ],
      "metadata": {
        "id": "cMC79VZXTP8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(items):\n",
        "  packed_tokens = torch.nn.utils.rnn.pack_sequence(items, enforce_sorted=False)\n",
        "\n",
        "  return packed_tokens"
      ],
      "metadata": {
        "id": "6bBsFmhETkif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4, \n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "z8YYsaroTanu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "CQ3sP7b1TtAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write down rnn model"
      ],
      "metadata": {
        "id": "KVFFYfgmRnyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN4GEN(torch.nn.Module):\n",
        "    def __init__(self, num_words, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = torch.nn.Embedding(num_words, hidden_size)\n",
        "        self.rnn = torch.nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, input_ids, lenght_inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: Torch.Tensor, shape: (seq_length, batch_size, hidden_size)\n",
        "\n",
        "        To understand inputs for this module, please check rnn_padding:\n",
        "            - https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
        "            - https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "        \"\"\"\n",
        "        embs = self.embeddings(input_ids)\n",
        "\n",
        "        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(embs, lenght_inputs, enforce_sorted=False)\n",
        "        rnn_outputs, _ = self.rnn(packed_sequences)\n",
        "        unpacked_sequences, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_outputs)\n",
        "        return unpacked_sequences @ self.embeddings.weight.T\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_one_token(self, input_ids):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: Torch.Tensor, shape: (seq_length, 1, hidden_size)\n",
        "        \"\"\"\n",
        "        embs = self.embeddings(input_ids)\n",
        "        rnn_outputs, _ = self.rnn(embs)\n",
        "\n",
        "        return rnn_outputs[-1] @ self.embeddings.weight.T\n"
      ],
      "metadata": {
        "id": "YzSGmgGSPY8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN4GEN(len(all_words) + 16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "knUGrNC7R5tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.predict_one_token(torch.tensor([1, 10]))\n",
        "output.shape"
      ],
      "metadata": {
        "id": "lxTu7smMZIDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(output, dim=0)"
      ],
      "metadata": {
        "id": "_8sMW0nOZ4j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, batch in enumerate(train_dataloader):\n",
        "    inputs_ = batch\n",
        "    inputs_, inputs_lenght_ = torch.nn.utils.rnn.pad_packed_sequence(inputs_)\n",
        "    inputs, labels = inputs_[:-1], inputs_[1:]\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs, inputs_lenght_ - 1)\n",
        "    loss = criterion(outputs.view(-1, len(all_words) + 16), labels.view(-1))\n",
        "    optimizer.step()\n",
        "    #print(f\"{idx:>10}: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "iCNIb21QR5v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test it with different generation strategies"
      ],
      "metadata": {
        "id": "cZQwTYZURnWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greed_search(model):\n",
        "    input = torch.tensor([1])\n",
        "    while input.size(0) < 16:\n",
        "     output = model.predict_one_token(input)\n",
        "     max_token = torch.max(output, dim=0)[1]\n",
        "     input = torch.concat([input, max_token.reshape(1)])\n",
        "     if max_token == 2:\n",
        "       break\n",
        "    return input"
      ],
      "metadata": {
        "id": "4IRWBrqLQzMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = greed_search(model)"
      ],
      "metadata": {
        "id": "zrdtS_jSa7MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenizer = {idx: w for idx, w in train_dataset.tokenizer.items()}"
      ],
      "metadata": {
        "id": "s4LVpPJ7bQzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[detokenizer.get(i, '') for i in ans]"
      ],
      "metadata": {
        "id": "mH4KKxETbm8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sample(model):\n",
        "    input = torch.tensor([1])\n",
        "    while input.size(0) < 16:\n",
        "     output = model.predict_one_token(input)\n",
        "     prob = torch.softmax(output, dim=0)\n",
        "     token = torch.multinomial(prob, 1)\n",
        "     input = torch.concat([input, token])\n",
        "     if token == 2:\n",
        "       break\n",
        "    return input"
      ],
      "metadata": {
        "id": "I485ZyRkQzJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = top_k_sample(model)"
      ],
      "metadata": {
        "id": "KuAyJLGcc22t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check generation"
      ],
      "metadata": {
        "id": "EgH2LUtgQzDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[detokenizer.get(i, '') for i in ans]"
      ],
      "metadata": {
        "id": "Tr-17i2kc77D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HR5qNj77c8I3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}